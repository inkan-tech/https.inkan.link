#!/bin/bash

# Website Performance Analysis Script for Inkan.link
# Comprehensive performance, SEO, and security analysis
# Compatible with Hugo static sites and bilingual content

set -e

# Configuration
SITE_URL="https://inkan.link"
REPORT_DIR="./performance-reports"
DATE=$(date +%Y%m%d_%H%M%S)
REPORT_FILE="$REPORT_DIR/analysis_$DATE.md"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Create report directory
mkdir -p "$REPORT_DIR"

# Initialize report
cat > "$REPORT_FILE" << EOF
# Website Performance Analysis Report
**Site:** $SITE_URL  
**Date:** $(date)  
**Generated by:** Inkan.link Performance Analysis Script

---

EOF

log_info "Starting comprehensive website analysis for $SITE_URL"

# 1. Site Accessibility Check
log_info "Running accessibility analysis..."
if command -v axe &> /dev/null; then
    axe "$SITE_URL" --save "$REPORT_DIR/accessibility_$DATE.json"
    log_success "Accessibility analysis completed"
    
    cat >> "$REPORT_FILE" << EOF
## Accessibility Analysis
- Report saved to: \`accessibility_$DATE.json\`
- Tool: axe-core CLI

EOF
else
    log_warning "axe-core CLI not found. Install with: npm install -g @axe-core/cli"
    cat >> "$REPORT_FILE" << EOF
## Accessibility Analysis
- ⚠️ axe-core CLI not available
- Install with: \`npm install -g @axe-core/cli\`

EOF
fi

# 2. Lighthouse Performance Audit
log_info "Running Lighthouse performance audit..."
if command -v lighthouse &> /dev/null; then
    # Desktop audit
    lighthouse "$SITE_URL" \
        --preset=desktop \
        --output=html \
        --output=json \
        --output-path="$REPORT_DIR/lighthouse_desktop_$DATE" \
        --chrome-flags="--headless --no-sandbox" \
        --quiet
    
    # Mobile audit
    lighthouse "$SITE_URL" \
        --form-factor=mobile \
        --output=html \
        --output=json \
        --output-path="$REPORT_DIR/lighthouse_mobile_$DATE" \
        --chrome-flags="--headless --no-sandbox" \
        --quiet
    
    log_success "Lighthouse audits completed"
    
    cat >> "$REPORT_FILE" << EOF
## Lighthouse Performance Audit
- Desktop report: \`lighthouse_desktop_$DATE.html\`
- Mobile report: \`lighthouse_mobile_$DATE.html\`
- JSON data available for both audits

EOF
else
    log_warning "Lighthouse CLI not found. Install with: npm install -g lighthouse"
    cat >> "$REPORT_FILE" << EOF
## Lighthouse Performance Audit
- ⚠️ Lighthouse CLI not available
- Install with: \`npm install -g lighthouse\`

EOF
fi

# 3. PageSpeed Insights API Check
log_info "Fetching PageSpeed Insights data..."
if command -v curl &> /dev/null; then
    # Desktop strategy
    PSI_DESKTOP=$(curl -s "https://www.googleapis.com/pagespeed/insights/v5/runPagespeed?url=$SITE_URL&strategy=desktop&category=performance&category=accessibility&category=best-practices&category=seo")
    echo "$PSI_DESKTOP" > "$REPORT_DIR/pagespeed_desktop_$DATE.json"
    
    # Mobile strategy  
    PSI_MOBILE=$(curl -s "https://www.googleapis.com/pagespeed/insights/v5/runPagespeed?url=$SITE_URL&strategy=mobile&category=performance&category=accessibility&category=best-practices&category=seo")
    echo "$PSI_MOBILE" > "$REPORT_DIR/pagespeed_mobile_$DATE.json"
    
    # Extract key metrics
    DESKTOP_SCORE=$(echo "$PSI_DESKTOP" | grep -o '"PERFORMANCE":[^}]*"score":[0-9.]*' | grep -o '[0-9.]*$' || echo "N/A")
    MOBILE_SCORE=$(echo "$PSI_MOBILE" | grep -o '"PERFORMANCE":[^}]*"score":[0-9.]*' | grep -o '[0-9.]*$' || echo "N/A")
    
    log_success "PageSpeed Insights data fetched"
    
    cat >> "$REPORT_FILE" << EOF
## PageSpeed Insights
- Desktop Performance Score: **${DESKTOP_SCORE}**
- Mobile Performance Score: **${MOBILE_SCORE}**
- Raw data saved to JSON files

EOF
else
    log_error "curl not available for PageSpeed Insights API"
fi

# 4. Security Headers Check
log_info "Checking security headers..."
if command -v curl &> /dev/null; then
    HEADERS=$(curl -I -s "$SITE_URL")
    
    # Check for important security headers
    CSP_HEADER=$(echo "$HEADERS" | grep -i "content-security-policy" || echo "Missing")
    HSTS_HEADER=$(echo "$HEADERS" | grep -i "strict-transport-security" || echo "Missing") 
    X_FRAME_HEADER=$(echo "$HEADERS" | grep -i "x-frame-options" || echo "Missing")
    X_CONTENT_TYPE_HEADER=$(echo "$HEADERS" | grep -i "x-content-type-options" || echo "Missing")
    
    cat >> "$REPORT_FILE" << EOF
## Security Headers Analysis
- **Content-Security-Policy:** $(echo "$CSP_HEADER" | head -1)
- **Strict-Transport-Security:** $(echo "$HSTS_HEADER" | head -1)  
- **X-Frame-Options:** $(echo "$X_FRAME_HEADER" | head -1)
- **X-Content-Type-Options:** $(echo "$X_CONTENT_TYPE_HEADER" | head -1)

EOF
    
    log_success "Security headers analysis completed"
fi

# 5. SSL/TLS Certificate Check
log_info "Checking SSL/TLS certificate..."
if command -v openssl &> /dev/null; then
    SSL_INFO=$(echo | openssl s_client -servername $(echo "$SITE_URL" | sed 's/https\?:\/\///') -connect $(echo "$SITE_URL" | sed 's/https\?:\/\///'):443 2>/dev/null | openssl x509 -noout -dates -subject -issuer 2>/dev/null || echo "SSL check failed")
    
    cat >> "$REPORT_FILE" << EOF
## SSL/TLS Certificate
\`\`\`
$SSL_INFO
\`\`\`

EOF
    
    log_success "SSL certificate check completed"
fi

# 6. Core Web Vitals Assessment
log_info "Assessing Core Web Vitals..."
if command -v node &> /dev/null; then
    # Create a simple Node.js script to extract CWV from PageSpeed Insights
    cat > "/tmp/extract_cwv.js" << 'NODEJS'
const fs = require('fs');

function extractCoreWebVitals(filePath, strategy) {
    try {
        const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));
        const audits = data.lighthouseResult.audits;
        
        const lcp = audits['largest-contentful-paint']?.displayValue || 'N/A';
        const fid = audits['max-potential-fid']?.displayValue || 'N/A';
        const cls = audits['cumulative-layout-shift']?.displayValue || 'N/A';
        const fcp = audits['first-contentful-paint']?.displayValue || 'N/A';
        
        console.log(`## Core Web Vitals (${strategy})`);
        console.log(`- **Largest Contentful Paint (LCP):** ${lcp}`);
        console.log(`- **First Input Delay (FID):** ${fid}`);
        console.log(`- **Cumulative Layout Shift (CLS):** ${cls}`);
        console.log(`- **First Contentful Paint (FCP):** ${fcp}`);
        console.log('');
    } catch (error) {
        console.log(`## Core Web Vitals (${strategy})`);
        console.log('- Data extraction failed');
        console.log('');
    }
}

const args = process.argv.slice(2);
if (args.length >= 2) {
    extractCoreWebVitals(args[0], args[1]);
}
NODEJS

    if [ -f "$REPORT_DIR/pagespeed_desktop_$DATE.json" ]; then
        node /tmp/extract_cwv.js "$REPORT_DIR/pagespeed_desktop_$DATE.json" "Desktop" >> "$REPORT_FILE"
    fi
    
    if [ -f "$REPORT_DIR/pagespeed_mobile_$DATE.json" ]; then
        node /tmp/extract_cwv.js "$REPORT_DIR/pagespeed_mobile_$DATE.json" "Mobile" >> "$REPORT_FILE"
    fi
    
    rm -f /tmp/extract_cwv.js
    log_success "Core Web Vitals assessment completed"
fi

# 7. Image Optimization Check
log_info "Checking image optimization..."
if command -v curl &> /dev/null && command -v grep &> /dev/null; then
    # Get page source and find images
    PAGE_SOURCE=$(curl -s "$SITE_URL")
    IMAGES=$(echo "$PAGE_SOURCE" | grep -oP 'src="[^"]*\.(jpg|jpeg|png|gif|webp|svg)"' | sed 's/src="//g' | sed 's/"//g' | head -10)
    
    cat >> "$REPORT_FILE" << EOF
## Image Optimization Check
Found images:
EOF
    
    IMAGE_COUNT=0
    for img in $IMAGES; do
        if [[ $img == http* ]]; then
            IMG_URL="$img"
        else
            IMG_URL="$SITE_URL$(echo "$img" | sed 's|^/||')"
        fi
        
        SIZE=$(curl -s -I "$IMG_URL" | grep -i content-length | awk '{print $2}' | tr -d '\r' || echo "Unknown")
        FORMAT=$(echo "$img" | grep -oP '\.[a-z]+$' | tr -d '.')
        
        echo "- \`$img\` - Size: ${SIZE} bytes - Format: ${FORMAT}" >> "$REPORT_FILE"
        
        IMAGE_COUNT=$((IMAGE_COUNT + 1))
        if [ $IMAGE_COUNT -ge 5 ]; then
            break
        fi
    done
    
    echo "" >> "$REPORT_FILE"
    log_success "Image optimization check completed"
fi

# 8. Sitemap and Robots.txt Check
log_info "Checking sitemap and robots.txt..."
SITEMAP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SITE_URL/sitemap.xml")
ROBOTS_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SITE_URL/robots.txt")

cat >> "$REPORT_FILE" << EOF
## SEO Infrastructure
- **Sitemap.xml:** HTTP $SITEMAP_STATUS $([ "$SITEMAP_STATUS" = "200" ] && echo "✅" || echo "❌")
- **Robots.txt:** HTTP $ROBOTS_STATUS $([ "$ROBOTS_STATUS" = "200" ] && echo "✅" || echo "❌")

EOF

log_success "SEO infrastructure check completed"

# 9. Bilingual Content Check (specific to Inkan.link)
log_info "Checking bilingual content structure..."
FR_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SITE_URL")
EN_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SITE_URL/en")

cat >> "$REPORT_FILE" << EOF
## Bilingual Content Structure
- **French (default):** HTTP $FR_STATUS $([ "$FR_STATUS" = "200" ] && echo "✅" || echo "❌")
- **English (/en):** HTTP $EN_STATUS $([ "$EN_STATUS" = "200" ] && echo "✅" || echo "❌")

EOF

log_success "Bilingual content check completed"

# 10. Generate Summary
cat >> "$REPORT_FILE" << EOF
---

## Summary
This comprehensive analysis was performed on $(date) for $SITE_URL.

### Key Performance Indicators
- Desktop Performance: $DESKTOP_SCORE
- Mobile Performance: $MOBILE_SCORE  
- Sitemap Available: $([ "$SITEMAP_STATUS" = "200" ] && echo "Yes" || echo "No")
- Bilingual Support: $([ "$EN_STATUS" = "200" ] && echo "Yes" || echo "No")

### Next Steps
1. Review Lighthouse reports for detailed recommendations
2. Check accessibility report for compliance issues
3. Implement missing security headers if needed
4. Optimize images based on the optimization check
5. Monitor Core Web Vitals regularly

### Tools Used
- Lighthouse CLI
- PageSpeed Insights API
- axe-core (accessibility)
- OpenSSL (certificate check)
- curl (HTTP checks)

**Report generated by Inkan.link Performance Analysis Script**
EOF

# Completion
log_success "Analysis completed! Report saved to: $REPORT_FILE"
log_info "Reports directory: $REPORT_DIR"

# Optional: Open report if on macOS
if [[ "$OSTYPE" == "darwin"* ]]; then
    log_info "Opening report..."
    open "$REPORT_FILE"
fi

echo ""
echo "📊 Performance Analysis Summary:"
echo "================================"
echo "🎯 Report: $REPORT_FILE"
echo "📁 Files: $REPORT_DIR/"
echo "🌐 Site: $SITE_URL"
echo "📅 Date: $(date)"
echo ""
echo "🚀 Run this script regularly to track performance improvements!"